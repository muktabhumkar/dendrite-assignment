{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOPlLP94rix2LXYB+N/tS04"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import json\n","import pandas as pd\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.pipeline import Pipeline\n","from sklearn.impute import SimpleImputer\n","from sklearn.decomposition import PCA\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import classification_report\n","from sklearn.feature_selection import SelectKBest, f_classif\n","from sklearn.compose import ColumnTransformer\n","from sklearn.preprocessing import StandardScaler\n","\n","# Function to parse JSON and execute ML pipeline\n","def run_ml_pipeline(json_file, csv_file):\n","    # Step 1: Read JSON file\n","    with open(json_file, 'r') as f:\n","        config = json.load(f)\n","\n","    # Step 2: Load dataset\n","    data = pd.read_csv(csv_file)\n","\n","    # Extract target and features from JSON\n","    target = config['target']\n","    features = config['features']\n","\n","    # Step 3: Handle missing values\n","    imputer = SimpleImputer(strategy='mean')\n","    data[features] = imputer.fit_transform(data[features])\n","\n","    # Step 4: Feature reduction\n","    reduction_type = config['feature_reduction']\n","    if reduction_type == 'No Reduction':\n","        X = data[features]\n","    elif reduction_type == 'Corr with Target':\n","        # Implement correlation-based feature selection\n","        corr = data.corr()[target].abs()\n","        relevant_features = corr[corr > 0.1].index.tolist()\n","        X = data[relevant_features]\n","    elif reduction_type == 'Tree-based':\n","        # Example using RandomForest for feature importance\n","        model = RandomForestClassifier()\n","        model.fit(data[features], data[target])\n","        importances = model.feature_importances_\n","        indices = importances.argsort()[::-1]\n","        X = data[features].iloc[:, indices[:5]]  # Select top 5 features\n","    elif reduction_type == 'PCA':\n","        pca = PCA(n_components=2)  # Adjust number of components as needed\n","        X = pca.fit_transform(data[features])\n","\n","    y = data[target]\n","\n","    # Step 5: Model building based on prediction type\n","    models = []\n","    for model_config in config['models']:\n","        if model_config['is_selected']:\n","            if model_config['type'] == 'RandomForest':\n","                model = RandomForestClassifier()\n","            elif model_config['type'] == 'LogisticRegression':\n","                model = LogisticRegression()\n","            else:\n","                continue\n","\n","            # Hyperparameter tuning with GridSearchCV\n","            param_grid = model_config['hyperparameters']\n","            grid_search = GridSearchCV(model, param_grid, cv=5)\n","            models.append((grid_search, model_config['type']))\n","\n","    # Step 6: Fit and predict\n","    for grid_search, model_type in models:\n","        grid_search.fit(X, y)\n","        predictions = grid_search.predict(X)\n","        print(f\"Model: {model_type}\")\n","        print(classification_report(y, predictions))\n","\n","# Example usage\n","run_ml_pipeline(r'C:\\Users\\mukta\\Downloads\\DS_Assignment - internship\\Screening Test - DS\\algoparams_from_ui.json.rtf', r'C:\\Users\\mukta\\Downloads\\DS_Assignment - internship\\Screening Test - DS/iris.csv')"],"metadata":{"id":"bR2Ot0nBNqrP","executionInfo":{"status":"ok","timestamp":1735054799026,"user_tz":-330,"elapsed":1829,"user":{"displayName":"Mukta Bhumkar","userId":"16030757974521127999"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"eE7XN_BxRHsK"},"execution_count":null,"outputs":[]}]}